{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TokaAyman/NLP_ITI/blob/main/lec3_assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9fa57d4c",
      "metadata": {
        "id": "9fa57d4c"
      },
      "source": [
        "# RNN  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2cc642b",
      "metadata": {
        "id": "b2cc642b"
      },
      "source": [
        "it stands for recurrent neural network, as the same weights are reused at each step and is designed for sequential models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0aa69252",
      "metadata": {
        "id": "0aa69252"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from tqdm.autonotebook import tqdm\n",
        "\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import imshow\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc41d9bf",
      "metadata": {
        "id": "cc41d9bf"
      },
      "outputs": [],
      "source": [
        "D = 28*28\n",
        "n = 256\n",
        "C = 1\n",
        "\n",
        "classes = 10\n",
        "\n",
        "model_regular = nn.Sequential(\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(D, n),\n",
        "    nn.Tanh(),\n",
        "    nn.Linear(n, n),\n",
        "    nn.Tanh(),\n",
        "    nn.Linear(n, n),\n",
        "    nn.Tanh(),\n",
        "    nn.Linear(n, classes),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a245bba3",
      "metadata": {
        "id": "a245bba3"
      },
      "outputs": [],
      "source": [
        "h_2 = nn.Linear(n, n)\n",
        "\n",
        "model_shared = nn.Sequential(\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(D, n),\n",
        "    nn.Tanh(), h_2,\n",
        "    nn.Tanh(), h_2,\n",
        "    nn.Tanh(),\n",
        "    nn.Linear(n, classes),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e66de7e",
      "metadata": {
        "id": "9e66de7e"
      },
      "source": [
        "# Lets build an RNN network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28d2cf3e",
      "metadata": {
        "id": "28d2cf3e"
      },
      "outputs": [],
      "source": [
        "zip_file_url = \"https://download.pytorch.org/tutorial/data.zip\"\n",
        "\n",
        "import requests, zipfile, io\n",
        "r = requests.get(zip_file_url)\n",
        "z = zipfile.ZipFile(io.BytesIO(r.content))\n",
        "z.extractall()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a517d0f",
      "metadata": {
        "id": "3a517d0f"
      },
      "outputs": [],
      "source": [
        "namge_language_data = {}\n",
        "\n",
        "#We will use some code to remove UNICODE tokens to make life easy for us processing wise\n",
        "#e.g., convert something like \"Ślusàrski\" to Slusarski\n",
        "import unicodedata\n",
        "import string\n",
        "\n",
        "all_letters = string.ascii_letters + \" .,;'\"\n",
        "n_letters = len(all_letters)\n",
        "alphabet = {}\n",
        "for i in range(n_letters):\n",
        "    alphabet[all_letters[i]] = i\n",
        "\n",
        "# Turn a Unicode string to plain ASCII, thanks to https://stackoverflow.com/a/518232/2809427\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "        and c in all_letters\n",
        "    )\n",
        "\n",
        "#Loop through every language, open the zip file entry, and read all the lines from the text file.\n",
        "for zip_path in z.namelist():\n",
        "    if \"data/names/\" in zip_path and zip_path.endswith(\".txt\"):\n",
        "        lang = zip_path[len(\"data/names/\"):-len(\".txt\")]\n",
        "        with z.open(zip_path) as myfile:\n",
        "            lang_names = [unicodeToAscii(line).lower() for line in str(myfile.read(), encoding='utf-8').strip().split(\"\\n\")]\n",
        "            namge_language_data[lang] = lang_names\n",
        "        print(lang, \": \", len(lang_names)) #Print out the name of each language too.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cca327b6",
      "metadata": {
        "id": "cca327b6"
      },
      "outputs": [],
      "source": [
        "namge_language_data.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "819596d3",
      "metadata": {
        "id": "819596d3"
      },
      "outputs": [],
      "source": [
        "# Show the first 10 Arabic names in the dataset\n",
        "print(\"First 10 Arabic names in the dataset:\")\n",
        "for i, name in enumerate(namge_language_data[\"Arabic\"][:10], 1):\n",
        "    print(f\"{i}. {name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nvTtG_QUjofY"
      },
      "outputs": [],
      "source": [
        "class LanguageNameDataset(Dataset):\n",
        "    '''\n",
        "    self.label_names becomes ['English', 'Spanish', 'Japanese']\n",
        "    self.data becomes ['Smith', 'Johnson', 'Williams', 'Garcia', 'Rodriguez', 'Lopez', 'Tanaka', 'Suzuki', 'Sato']\n",
        "    self.labels becomes [0, 0, 0, 1, 1, 1, 2, 2, 2] (0 for English, 1 for Spanish, 2 for Japanese)\n",
        "\n",
        "    Now, let's see what happens when we access a specific item, for example dataset[4]:\n",
        "\n",
        "    The __getitem__ method is called with idx=4\n",
        "    It retrieves name = self.data[4] which is 'Rodriguez'\n",
        "    It retrieves label = self.labels[4] which is 1 (Spanish)\n",
        "    It calls self.string2InputVec('Rodriguez') which does the following:\n",
        "\n",
        "    Creates a tensor of zeros with length 9 (number of characters in \"Rodriguez\")\n",
        "    For each character, it replaces the zeros with the corresponding integer from the vocabulary:\n",
        "\n",
        "    'R' → 44\n",
        "    'o' → 15\n",
        "    'd' → 4\n",
        "    'r' → 18\n",
        "    'i' → 9\n",
        "    'g' → 7\n",
        "    'u' → 21\n",
        "    'e' → 5\n",
        "    'z' → 26\n",
        "\n",
        "\n",
        "    This creates the tensor: tensor([44, 15, 4, 18, 9, 7, 21, 5, 26])\n",
        "\n",
        "\n",
        "    It converts the label 1 to a tensor: tensor([1])\n",
        "    It returns the tuple: (tensor([44, 15, 4, 18, 9, 7, 21, 5, 26]), tensor([1]))\n",
        "    '''\n",
        "\n",
        "    def __init__(self, lang_name_dict, vocabulary):\n",
        "        self.label_names = [x for x in lang_name_dict.keys()]\n",
        "        self.data = []\n",
        "        self.labels = []\n",
        "        self.vocabulary = vocabulary\n",
        "        for y, language in enumerate(self.label_names):\n",
        "            for sample in lang_name_dict[language]:\n",
        "                self.data.append(sample)\n",
        "                self.labels.append(y)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def string2InputVec(self, input_string):\n",
        "        \"\"\"\n",
        "        This method will convert any input string into a vector of long values, according to the vocabulary used by this object.\n",
        "        input_string: the string to convert to a tensor\n",
        "        \"\"\"\n",
        "        T = len(input_string) #How many characters long is the string?\n",
        "\n",
        "        #Create a new tensor to store the result in\n",
        "        name_vec = torch.zeros((T), dtype=torch.long)\n",
        "        #iterate through the string and place the appropriate values into the tensor\n",
        "        for pos, character in enumerate(input_string):\n",
        "            name_vec[pos] = self.vocabulary[character]\n",
        "\n",
        "        return name_vec\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        name = self.data[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # Convert the correct class label into a tensor for PyTorch\n",
        "        label_vec = torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "\n",
        "        return self.string2InputVec(name), label\n"
      ],
      "id": "nvTtG_QUjofY"
    },
    {
      "cell_type": "markdown",
      "id": "b3440164",
      "metadata": {
        "id": "b3440164"
      },
      "source": [
        "edit hereeeeeeeeeeeeeeeeeeee"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "def collate_fn(batch):\n",
        "    sequences, labels = zip(*batch)\n",
        "    # Pad the sequences to the same length\n",
        "    padded_seqs = pad_sequence(sequences, batch_first=True, padding_value=0)\n",
        "    return padded_seqs, torch.tensor(labels)"
      ],
      "metadata": {
        "id": "Fm36qvZCkJ8J"
      },
      "id": "Fm36qvZCkJ8J",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pNAK0j5xjofY"
      },
      "outputs": [],
      "source": [
        "dataset = LanguageNameDataset(namge_language_data, alphabet)\n",
        "\n",
        "\n",
        "train_data, test_data = torch.utils.data.random_split(dataset, (len(dataset)-300, 300))\n",
        "train_loader = DataLoader(train_data, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_data, batch_size=2, shuffle=False, collate_fn=collate_fn)"
      ],
      "id": "pNAK0j5xjofY"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14613ff1",
      "metadata": {
        "id": "14613ff1"
      },
      "outputs": [],
      "source": [
        "# example of embedding\n",
        "# ammae\n",
        "'input sequence with T = 5 items but a vocabulary of only 3 items'\n",
        "with torch.no_grad():\n",
        "    input_sequence = torch.tensor([0, 1, 1, 0, 2], dtype=torch.long)\n",
        "    embd = nn.Embedding(3, 2)\n",
        "    x_seq = embd(input_sequence)\n",
        "    print(input_sequence.shape, x_seq.shape)\n",
        "    print(x_seq)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9f0cfbf",
      "metadata": {
        "id": "a9f0cfbf"
      },
      "outputs": [],
      "source": [
        "class LastTimeStep(nn.Module):\n",
        "    \"\"\"\n",
        "    A class for extracting the hidden activations of the last time step following\n",
        "    the output of a PyTorch RNN module.\n",
        "    it extracts the final hidden state from the RNN,\n",
        "    which contains the model's \"understanding\" of the entire name sequence\n",
        "    \"\"\"\n",
        "    def __init__(self, rnn_layers=1, bidirectional=False):\n",
        "        super(LastTimeStep, self).__init__()\n",
        "        self.rnn_layers = rnn_layers\n",
        "        if bidirectional:\n",
        "            self.num_driections = 2\n",
        "        else:\n",
        "            self.num_driections = 1\n",
        "\n",
        "    def forward(self, input):\n",
        "        #Result is either a tuple (out, h_t)\n",
        "        #or a tuple (out, (h_t, c_t))\n",
        "        rnn_output = input[0]\n",
        "        last_step = input[1] #this will be h_t\n",
        "        if(type(last_step) == tuple): # unless it's a tuple,\n",
        "            last_step = last_step[0] # then h_t is the first item in the tuple\n",
        "        batch_size = last_step.shape[1] # per docs, shape is: '(num_layers * num_directions, batch, hidden_size)'\n",
        "        # reshaping so that everything is separate\n",
        "        last_step = last_step.view(self.rnn_layers, self.num_driections, batch_size, -1)\n",
        "        # We want the last layer's results\n",
        "        last_step = last_step[self.rnn_layers-1]\n",
        "        # Re order so batch comes first\n",
        "        last_step = last_step.permute(1, 0, 2)\n",
        "        # Finally, flatten the last two dimensions into one\n",
        "        return last_step.reshape(batch_size, -1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3fdebd0",
      "metadata": {
        "id": "e3fdebd0"
      },
      "outputs": [],
      "source": [
        "D = 64\n",
        "vocab_size = len(all_letters)\n",
        "hidden_nodes = 256\n",
        "classes = len(dataset.label_names)\n",
        "\n",
        "first_rnn = nn.Sequential(\n",
        "  nn.Embedding(vocab_size, D), #(B, T) -> (B, T, D)\n",
        "  nn.RNN(D, hidden_nodes, batch_first=True), #(B, T, D) -> ( (B,T,D) , (S, B, D)  )\n",
        "  # the tanh activation is built into the RNN object, so we don't need to do it here\n",
        "  LastTimeStep(), # We need to take the RNN output and reduce it to one item, (B, D)\n",
        "  nn.Linear(hidden_nodes, classes), #(B, D) -> (B, classes)\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d05214f0",
      "metadata": {
        "id": "d05214f0"
      },
      "outputs": [],
      "source": [
        "def moveTo(obj, device):\n",
        "    \"\"\"\n",
        "    obj: the python object to move to a device, or to move its contents to a device\n",
        "    device: the compute device to move objects to\n",
        "    \"\"\"\n",
        "    if hasattr(obj, \"to\"):\n",
        "        return obj.to(device)\n",
        "    elif isinstance(obj, list):\n",
        "        return [moveTo(x, device) for x in obj]\n",
        "    elif isinstance(obj, tuple):\n",
        "        return tuple(moveTo(list(obj), device))\n",
        "    elif isinstance(obj, set):\n",
        "        return set(moveTo(list(obj), device))\n",
        "    elif isinstance(obj, dict):\n",
        "        to_ret = dict()\n",
        "        for key, value in obj.items():\n",
        "            to_ret[moveTo(key, device)] = moveTo(value, device)\n",
        "        return to_ret\n",
        "    else:\n",
        "        return obj\n",
        "\n",
        "def run_epoch(model, optimizer, data_loader, loss_func, device, results, score_funcs, prefix=\"\", desc=None):\n",
        "    \"\"\"\n",
        "    model -- the PyTorch model / \"Module\" to run for one epoch\n",
        "    optimizer -- the object that will update the weights of the network\n",
        "    data_loader -- DataLoader object that returns tuples of (input, label) pairs.\n",
        "    loss_func -- the loss function that takes in two arguments, the model outputs and the labels, and returns a score\n",
        "    device -- the compute lodation to perform training\n",
        "    score_funcs -- a dictionary of scoring functions to use to evalue the performance of the model\n",
        "    prefix -- a string to pre-fix to any scores placed into the _results_ dictionary.\n",
        "    desc -- a description to use for the progress bar.\n",
        "    \"\"\"\n",
        "    running_loss = []\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "    start = time.time()\n",
        "    for inputs, labels in tqdm(data_loader, desc=desc, leave=False):\n",
        "        #Move the batch to the device we are using.\n",
        "        inputs = moveTo(inputs, device)\n",
        "        labels = moveTo(labels, device)\n",
        "\n",
        "        y_hat = model(inputs) #this just computed f_Θ(x(i))\n",
        "        # Compute loss.\n",
        "        loss = loss_func(y_hat, labels)\n",
        "\n",
        "        if model.training:\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        #Now we are just grabbing some information we would like to have\n",
        "        running_loss.append(loss.item())\n",
        "\n",
        "        if len(score_funcs) > 0 and isinstance(labels, torch.Tensor):\n",
        "            #moving labels & predictions back to CPU for computing / storing predictions\n",
        "            labels = labels.detach().cpu().numpy()\n",
        "            y_hat = y_hat.detach().cpu().numpy()\n",
        "            #add to predictions so far\n",
        "            y_true.extend(labels.tolist())\n",
        "            y_pred.extend(y_hat.tolist())\n",
        "    #end training epoch\n",
        "    end = time.time()\n",
        "\n",
        "    y_pred = np.asarray(y_pred)\n",
        "    if len(y_pred.shape) == 2 and y_pred.shape[1] > 1: #We have a classification problem, convert to labels\n",
        "        y_pred = np.argmax(y_pred, axis=1)\n",
        "    #Else, we assume we are working on a regression problem\n",
        "\n",
        "    results[prefix + \" loss\"].append( np.mean(running_loss) )\n",
        "    for name, score_func in score_funcs.items():\n",
        "        try:\n",
        "            results[prefix + \" \" + name].append( score_func(y_true, y_pred) )\n",
        "        except:\n",
        "            results[prefix + \" \" + name].append(float(\"NaN\"))\n",
        "    return end-start #time spent on epoch\n",
        "\n",
        "def train_simple_network(model, loss_func, train_loader, test_loader=None, score_funcs=None,\n",
        "                         epochs=50, device=\"cpu\", checkpoint_file=None, lr=0.001):\n",
        "    \"\"\"Train simple neural networks\n",
        "\n",
        "    Keyword arguments:\n",
        "    model -- the PyTorch model / \"Module\" to train\n",
        "    loss_func -- the loss function that takes in batch in two arguments, the model outputs and the labels, and returns a score\n",
        "    train_loader -- PyTorch DataLoader object that returns tuples of (input, label) pairs.\n",
        "    test_loader -- Optional PyTorch DataLoader to evaluate on after every epoch\n",
        "    score_funcs -- A dictionary of scoring functions to use to evalue the performance of the model\n",
        "    epochs -- the number of training epochs to perform\n",
        "    device -- the compute lodation to perform training\n",
        "\n",
        "    \"\"\"\n",
        "    to_track = [\"epoch\", \"total time\", \"train loss\"]\n",
        "    if test_loader is not None:\n",
        "        to_track.append(\"test loss\")\n",
        "    for eval_score in score_funcs:\n",
        "        to_track.append(\"train \" + eval_score )\n",
        "        if test_loader is not None:\n",
        "            to_track.append(\"test \" + eval_score )\n",
        "\n",
        "    total_train_time = 0 #How long have we spent in the training loop?\n",
        "    results = {}\n",
        "    #Initialize every item with an empty list\n",
        "    for item in to_track:\n",
        "        results[item] = []\n",
        "\n",
        "    #SGD is Stochastic Gradient Decent.\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
        "    #Place the model on the correct compute resource (CPU or GPU)\n",
        "    model.to(device)\n",
        "    for epoch in tqdm(range(epochs), desc=\"Epoch\"):\n",
        "        model = model.train()#Put our model in training mode\n",
        "\n",
        "        total_train_time += run_epoch(model, optimizer, train_loader, loss_func, device, results, score_funcs, prefix=\"train\", desc=\"Training\")\n",
        "\n",
        "        results[\"total time\"].append( total_train_time )\n",
        "        results[\"epoch\"].append( epoch )\n",
        "\n",
        "        if test_loader is not None:\n",
        "            model = model.eval()\n",
        "            with torch.no_grad():\n",
        "                run_epoch(model, optimizer, test_loader, loss_func, device, results, score_funcs, prefix=\"test\", desc=\"Testing\")\n",
        "\n",
        "    if checkpoint_file is not None:\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'results' : results\n",
        "            }, checkpoint_file)\n",
        "\n",
        "    return pd.DataFrame.from_dict(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6381f6ca",
      "metadata": {
        "id": "6381f6ca"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Check if CUDA is available\n",
        "if torch.cuda.is_available():\n",
        "    print(\"CUDA is available. Using GPU.\")\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    print(\"CUDA is not available. Using CPU.\")\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "batch_one_train = train_simple_network(first_rnn,\n",
        "                                    loss_func,\n",
        "                                    train_loader,\n",
        "                                    test_loader=test_loader,\n",
        "                                    score_funcs={'Accuracy': accuracy_score},\n",
        "                                    device='cpu', epochs=5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9c0a98c",
      "metadata": {
        "id": "f9c0a98c"
      },
      "outputs": [],
      "source": [
        "sns.lineplot(x='epoch', y='test Accuracy', data=batch_one_train, label='RNN')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3bf5fcee",
      "metadata": {
        "id": "3bf5fcee"
      },
      "outputs": [],
      "source": [
        "pred_rnn = first_rnn.to(\"cpu\").eval()\n",
        "with torch.inference_mode():\n",
        "    preds = F.softmax(pred_rnn(dataset.string2InputVec(\"frank\").reshape(1,-1)), dim=-1)\n",
        "    for class_id in range(len(dataset.label_names)):\n",
        "        print(dataset.label_names[class_id], \":\", preds[0,class_id].item()*100 , \"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9033d2e",
      "metadata": {
        "id": "a9033d2e"
      },
      "source": [
        "### More simple example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b2ace22",
      "metadata": {
        "id": "0b2ace22"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Sample data: 3 batches, each with 5 names\n",
        "names_batch = [\n",
        "    # Batch 1: 5 American names\n",
        "    [\"John\", \"Elizabeth\", \"Michael\", \"Jennifer\", \"Christopher\"],\n",
        "\n",
        "    # Batch 2: 5 Japanese names\n",
        "    [\"Hiroshi\", \"Yuki\", \"Takeshi\", \"Aiko\", \"Kenji\"],\n",
        "\n",
        "    # Batch 3: 5 Indian names\n",
        "    [\"Raj\", \"Priya\", \"Aditya\", \"Divya\", \"Vikram\"]\n",
        "]\n",
        "\n",
        "# Step 1: Convert characters to indices\n",
        "# First, create a vocabulary of all characters\n",
        "all_chars = set()\n",
        "for batch in names_batch:\n",
        "    for name in batch:\n",
        "        all_chars.update(name)\n",
        "\n",
        "char_to_idx = {char: i+1 for i, char in enumerate(sorted(all_chars))}\n",
        "\n",
        "# Add padding token\n",
        "char_to_idx['<PAD>'] = 0\n",
        "idx_to_char = {i: char for char, i in char_to_idx.items()}\n",
        "\n",
        "print(\"Character vocabulary size:\", len(char_to_idx))\n",
        "print(\"Character mapping example:\", {k: char_to_idx[k] for k in list(char_to_idx.keys())[:5]})\n",
        "\n",
        "# Step 2: Find the longest name to determine max sequence length\n",
        "max_name_length = max(len(name) for batch in names_batch for name in batch)\n",
        "print(\"Maximum name length:\", max_name_length)\n",
        "\n",
        "# Step 3: Convert names to padded sequences of indices\n",
        "# Shape will be [batch_size, seq_length, input_size]\n",
        "# Where input_size=1 (one character at a time)\n",
        "input_data = torch.zeros(3, 5, max_name_length, dtype=torch.long)\n",
        "\n",
        "for batch_idx, batch in enumerate(names_batch):\n",
        "    for name_idx, name in enumerate(batch):\n",
        "        for char_idx, char in enumerate(name):\n",
        "            input_data[batch_idx, name_idx, char_idx] = char_to_idx[char]\n",
        "\n",
        "print(\"Input tensor shape:\", input_data.shape)\n",
        "print(\"\\nEncoded first name in batch 1:\", input_data[0, 0])\n",
        "print(\"Decoded back:\", ''.join([idx_to_char[idx.item()] for idx in input_data[0, 0] if idx.item() > 0]))\n",
        "\n",
        "# Step 4: For RNN processing, we need to convert this to one-hot encoding or embeddings\n",
        "# Let's use embeddings which is more efficient\n",
        "vocab_size = len(char_to_idx)\n",
        "embedding_dim = 10  # Size of character embeddings\n",
        "\n",
        "class NameRNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_size):\n",
        "        super(NameRNN, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.rnn = nn.RNN(embedding_dim, hidden_size, batch_first=True)\n",
        "        # We'll use the final hidden state for classification\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: [batch_size, seq_length] for each name\n",
        "        embedded = self.embedding(x)  # Shape: [batch_size, seq_length, embedding_dim]\n",
        "        output, hidden = self.rnn(embedded)\n",
        "        return output, hidden\n",
        "\n",
        "# For processing, we'd need to reshape our data\n",
        "# Let's process one batch at a time for clarity\n",
        "model = NameRNN(vocab_size, embedding_dim, hidden_size=20)\n",
        "\n",
        "# Process the first name in each batch\n",
        "first_names = input_data[:, 0, :]  # Shape: [3, max_name_length]\n",
        "output, hidden = model(first_names)\n",
        "\n",
        "print(\"\\nOutput shape for first names:\", output.shape)  # Should be [3, max_name_length, 20]\n",
        "print(\"Hidden state shape:\", hidden.shape)  # Should be [1, 3, 20]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94398a55",
      "metadata": {
        "id": "94398a55"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}